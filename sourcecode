# -*- coding: utf-8 -*-
"""NM-project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YGn3JEk-32stjyoFW7o9PFMOj3maRyHS
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import joblib
import os

class AirQualityPredictor:
    """
    A machine learning system for predicting air quality levels and providing environmental insights.
    """

    def __init__(self):
        self.models = {
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingRegressor(random_state=42),
            'linear_regression': LinearRegression()
        }
        self.best_model = None
        self.best_model_name = None
        self.scaler = StandardScaler()
        self.feature_importance = None

    def load_data(self, file_path=None):
        """
        Load air quality data from a file or use sample data.

        Parameters:
        -----------
        file_path : str, optional
            Path to the CSV file containing air quality data

        Returns:
        --------
        pandas.DataFrame
            The loaded data
        """
        if file_path and os.path.exists(file_path):
            print(f"Loading data from {file_path}")
            data = pd.read_csv(file_path)
        else:
            print("No valid file path provided. Using sample data...")
            # Generate sample data
            np.random.seed(42)
            n_samples = 1000

            # Generate features that would be typical in air quality monitoring
            temperature = np.random.normal(25, 5, n_samples)  # in Celsius
            humidity = np.random.normal(60, 15, n_samples)    # in percentage
            wind_speed = np.random.normal(10, 5, n_samples)   # in km/h
            pressure = np.random.normal(1013, 10, n_samples)  # in hPa
            traffic_density = np.random.normal(500, 200, n_samples)  # vehicles/hour
            industrial_activity = np.random.normal(50, 20, n_samples)  # arbitrary scale

            # Generate target variables - PM2.5, PM10, NO2, O3, CO
            # Formula is simplified but introduces relationships between features and targets
            pm25 = 5 + 0.1*temperature + 0.05*humidity - 0.2*wind_speed + 0.01*traffic_density + 0.02*industrial_activity + np.random.normal(0, 2, n_samples)
            pm10 = 15 + 0.15*temperature + 0.08*humidity - 0.25*wind_speed + 0.02*traffic_density + 0.03*industrial_activity + np.random.normal(0, 3, n_samples)
            no2 = 20 + 0.05*temperature - 0.1*wind_speed + 0.03*traffic_density + 0.02*industrial_activity + np.random.normal(0, 4, n_samples)
            o3 = 30 + 0.2*temperature - 0.05*humidity - 0.1*pressure + np.random.normal(0, 5, n_samples)
            co = 0.5 + 0.01*traffic_density + 0.02*industrial_activity - 0.05*wind_speed + np.random.normal(0, 0.1, n_samples)

            # Ensure all values are positive
            pm25 = np.maximum(0, pm25)
            pm10 = np.maximum(0, pm10)
            no2 = np.maximum(0, no2)
            o3 = np.maximum(0, o3)
            co = np.maximum(0, co)

            # Create dataframe
            data = pd.DataFrame({
                'temperature': temperature,
                'humidity': humidity,
                'wind_speed': wind_speed,
                'pressure': pressure,
                'traffic_density': traffic_density,
                'industrial_activity': industrial_activity,
                'PM2.5': pm25,
                'PM10': pm10,
                'NO2': no2,
                'O3': o3,
                'CO': co
            })

        print(f"Data loaded with {data.shape[0]} samples and {data.shape[1]} features")
        return data

    def preprocess_data(self, data, target='PM2.5'):
        """
        Preprocess the data and split it into training and testing sets.

        Parameters:
        -----------
        data : pandas.DataFrame
            The air quality data
        target : str, default='PM2.5'
            The target variable to predict

        Returns:
        --------
        tuple
            (X_train, X_test, y_train, y_test)
        """
        print(f"Preprocessing data with target: {target}")

        # Check if target is in the dataframe
        if target not in data.columns:
            raise ValueError(f"Target {target} not found in the data")

        # Separate features and target
        X = data.drop([col for col in ['PM2.5', 'PM10', 'NO2', 'O3', 'CO'] if col in data.columns], axis=1)
        y = data[target]

        # Handle missing values
        X = X.fillna(X.mean())

        # Split the data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Scale the features
        self.scaler.fit(X_train)
        X_train_scaled = self.scaler.transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        print(f"Data split into {X_train.shape[0]} training samples and {X_test.shape[0]} testing samples")

        return X_train, X_test, X_train_scaled, X_test_scaled, y_train, y_test, X.columns

    def train_models(self, X_train_scaled, y_train):
        """
        Train multiple machine learning models.

        Parameters:
        -----------
        X_train_scaled : numpy.ndarray
            Scaled training features
        y_train : numpy.ndarray
            Training target values
        """
        print("Training models...")
        model_metrics = {}

        for name, model in self.models.items():
            print(f"Training {name}...")
            model.fit(X_train_scaled, y_train)

        print("All models trained successfully")

    def evaluate_models(self, X_test_scaled, y_test):
        """
        Evaluate all trained models and select the best one.

        Parameters:
        -----------
        X_test_scaled : numpy.ndarray
            Scaled testing features
        y_test : numpy.ndarray
            Testing target values

        Returns:
        --------
        dict
            Performance metrics for each model
        """
        print("Evaluating models...")
        model_metrics = {}

        for name, model in self.models.items():
            y_pred = model.predict(X_test_scaled)

            # Calculate performance metrics
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            model_metrics[name] = {
                'MSE': mse,
                'RMSE': rmse,
                'MAE': mae,
                'R2': r2
            }

            print(f"{name} - RMSE: {rmse:.2f}, R2: {r2:.2f}")

        # Select the best model based on R2 score
        self.best_model_name = max(model_metrics, key=lambda k: model_metrics[k]['R2'])
        self.best_model = self.models[self.best_model_name]

        print(f"Best model: {self.best_model_name} with R2 score of {model_metrics[self.best_model_name]['R2']:.2f}")

        # Get feature importance for the best model if available
        if hasattr(self.best_model, 'feature_importances_'):
            self.feature_importance = self.best_model.feature_importances_

        return model_metrics

    def predict(self, input_data):
        """
        Make air quality predictions using the best model.

        Parameters:
        -----------
        input_data : pandas.DataFrame or dict
            New data for which predictions are to be made

        Returns:
        --------
        numpy.ndarray
            Predicted air quality values
        """
        if self.best_model is None:
            raise ValueError("Model has not been trained yet")

        # Convert dict to DataFrame if necessary
        if isinstance(input_data, dict):
            input_data = pd.DataFrame([input_data])

        # Scale the input data
        input_scaled = self.scaler.transform(input_data)

        # Make predictions
        predictions = self.best_model.predict(input_scaled)

        return predictions

    def save_model(self, path="air_quality_model.pkl"):
        """
        Save the trained model to a file.

        Parameters:
        -----------
        path : str, default="air_quality_model.pkl"
            Path where the model will be saved
        """
        if self.best_model is None:
            raise ValueError("No trained model to save")

        model_data = {
            'model': self.best_model,
            'scaler': self.scaler,
            'model_name': self.best_model_name,
            'feature_importance': self.feature_importance
        }

        joblib.dump(model_data, path)
        print(f"Model saved to {path}")

    def load_model(self, path="air_quality_model.pkl"):
        """
        Load a trained model from a file.

        Parameters:
        -----------
        path : str, default="air_quality_model.pkl"
            Path from where the model will be loaded
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file {path} not found")

        model_data = joblib.load(path)

        self.best_model = model_data['model']
        self.scaler = model_data['scaler']
        self.best_model_name = model_data['model_name']
        self.feature_importance = model_data['feature_importance']

        print(f"Model loaded from {path}")

    def get_environmental_insights(self, feature_names):
        """
        Generate environmental insights based on the trained model.

        Parameters:
        -----------
        feature_names : list
            Names of the features used in the model

        Returns:
        --------
        dict
            Environmental insights
        """
        if self.feature_importance is None:
            return {"error": "Feature importance not available for this model"}

        # Create a DataFrame of feature importances
        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Importance': self.feature_importance
        })

        # Sort by importance
        importance_df = importance_df.sort_values('Importance', ascending=False)

        # Generate insights
        top_factors = importance_df.head(3)['Feature'].tolist()

        insights = {
            "most_influential_factors": top_factors,
            "factor_importance": importance_df.set_index('Feature')['Importance'].to_dict(),
            "interpretation": f"The most significant factors affecting air quality are {', '.join(top_factors)}. "
                            f"This suggests that focusing on controlling these factors could have the greatest impact on improving air quality."
        }

        return insights

    def visualize_results(self, y_test, y_pred, feature_names=None):
        """
        Create visualizations of model performance and feature importance.

        Parameters:
        -----------
        y_test : numpy.ndarray
            Actual target values
        y_pred : numpy.ndarray
            Predicted target values
        feature_names : list, optional
            Names of the features used in the model
        """
        plt.figure(figsize=(12, 10))

        # Actual vs Predicted plot
        plt.subplot(2, 2, 1)
        plt.scatter(y_test, y_pred, alpha=0.5)
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
        plt.xlabel('Actual')
        plt.ylabel('Predicted')
        plt.title('Actual vs Predicted')

        # Residuals plot
        plt.subplot(2, 2, 2)
        residuals = y_test - y_pred
        plt.scatter(y_pred, residuals, alpha=0.5)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('Predicted')
        plt.ylabel('Residuals')
        plt.title('Residuals Plot')

        # Feature importance plot
        if self.feature_importance is not None and feature_names is not None:
            plt.subplot(2, 2, 3)
            importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': self.feature_importance
            }).sort_values('Importance', ascending=False)

            sns.barplot(x='Importance', y='Feature', data=importance_df)
            plt.title('Feature Importance')

        # Histogram of predictions
        plt.subplot(2, 2, 4)
        plt.hist(y_pred, bins=20, alpha=0.5, label='Predicted')
        plt.hist(y_test, bins=20, alpha=0.5, label='Actual')
        plt.xlabel('Value')
        plt.ylabel('Frequency')
        plt.title('Distribution of Actual vs Predicted Values')
        plt.legend()

        plt.tight_layout()
        plt.show()


# Main execution function
def main():
    print("===== Air Quality Prediction System =====")
    print("This system uses machine learning to predict air quality levels")
    print("and provide environmental insights.")

    # Initialize the predictor
    predictor = AirQualityPredictor()

    # Load data
    data = predictor.load_data()

    # Select target pollutant
    print("\nAvailable targets for prediction:")
    pollutants = ['PM2.5', 'PM10', 'NO2', 'O3', 'CO']
    for i, pollutant in enumerate(pollutants):
        print(f"{i+1}. {pollutant}")

    target_idx = int(input("\nSelect a pollutant to predict (1-5): ")) - 1
    target = pollutants[target_idx]

    # Preprocess data
    X_train, X_test, X_train_scaled, X_test_scaled, y_train, y_test, feature_names = predictor.preprocess_data(data, target)

    # Train models
    predictor.train_models(X_train_scaled, y_train)

    # Evaluate models
    metrics = predictor.evaluate_models(X_test_scaled, y_test)

    # Make predictions with the best model
    y_pred = predictor.predict(X_test)

    # Visualize results
    predictor.visualize_results(y_test, y_pred, feature_names)

    # Get environmental insights
    insights = predictor.get_environmental_insights(feature_names)

    print("\n===== Environmental Insights =====")
    print(f"Most influential factors: {', '.join(insights['most_influential_factors'])}")
    print(f"\nInterpretation: {insights['interpretation']}")

    # Interactive prediction
    print("\n===== Make New Predictions =====")
    print("Enter values for each feature (press Enter to use average):")

    new_data = {}
    for feature in feature_names:
        value = input(f"{feature} (avg={X_train[feature].mean():.2f}): ")
        if value.strip() == "":
            new_data[feature] = X_train[feature].mean()
        else:
            new_data[feature] = float(value)

    prediction = predictor.predict(new_data)[0]

    print(f"\nPredicted {target} level: {prediction:.2f}")

    # Interpret the prediction
    aqi_categories = [
        (0, 50, "Good"),
        (51, 100, "Moderate"),
        (101, 150, "Unhealthy for Sensitive Groups"),
        (151, 200, "Unhealthy"),
        (201, 300, "Very Unhealthy"),
        (301, 500, "Hazardous")
    ]

    # Simple mapping for demo purposes - in real application this would be more sophisticated
    aqi_value = prediction * 2 if target == "PM2.5" else prediction  # Simple scaling for demo

    category = "Unknown"
    for low, high, cat in aqi_categories:
        if low <= aqi_value <= high:
            category = cat
            break

    print(f"Air Quality Category: {category}")

    # Save the model
    save_model = input("\nDo you want to save this model? (y/n): ").lower()
    if save_model == 'y':
        predictor.save_model()

    print("\nThank you for using the Air Quality Prediction System!")


if __name__ == "__main__":
    main()